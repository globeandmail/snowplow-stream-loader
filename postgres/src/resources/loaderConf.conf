# Copyright (c) 2014-2016 Snowplow Analytics Ltd. All rights reserved.
#
# This program is licensed to you under the Apache License Version 2.0, and
# you may not use this file except in compliance with the Apache License
# Version 2.0.  You may obtain a copy of the Apache License Version 2.0 at
# http://www.apache.org/licenses/LICENSE-2.0.
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the Apache License Version 2.0 is distributed on an "AS
# IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
# implied.  See the Apache License Version 2.0 for the specific language
# governing permissions and limitations there under.

# This file (config.hocon.sample) contains a template with
# configuration options for the Elasticsearch Loader.

# Sources currently supported are:
# "kinesis" for reading records from a Kinesis stream
# "stdin" for reading unencoded tab-separated events from stdin
# If set to "stdin", JSON documents will not be sent to Elasticsearch
# but will be written to stdout.
# "nsq" for reading unencoded tab-separated events from NSQ
source = "kafka"
source = ${?QUEUE}

# Where to write good and bad records
sink {
  # Sinks currently supported are:
  # "elasticsearch" for writing good records to Elasticsearch
  # "stdout" for writing good records to stdout
  # "kinesis" for writing good records to kinesis
  good = "postgres"
  good = ${?GOOD_SINK}

  # Sinks currently supported are:
  # "kinesis" for writing bad records to Kinesis
  # "stderr" for writing bad records to stderr
  # "nsq" for writing bad records to NSQ
  # "none" for ignoring bad records
  bad="kafka"
  bad = ${?BAD_SINK}
}

# "good" for a stream of successfully enriched events
# "bad" for a stream of bad events
# "plain-json" for writing plain json
enabled = "good"
enabled=${?INPUT_QUEUE_TYPE}

aws {
  accessKey = iam
  secretKey = iam

  # Optional:
  #  arnRole = "arn:aws:iam::{accountId}:role/{role}"
  #  stsRegion = us-east-1
  #  arnRole can be added to enable assume role, make sure to include stsRegion in this case
  # arnRole = "{{arnRole}}"
  # stsRegion = "{{stsRegion}}"
}

kafka {
  broker = "0.0.0.0:9092"
  groupId = "abc"
  consumeTopic = "outEnriched"
  badProducerTopic = "badLoaderTopic"

}



queueConfig {
  enabled = "kafka"
  broker = "0.0.0.0:9092"
  #broker = ${?}
  groupId = "abc"
  consumeTopic = "outEnriched"
  badProducerTopic = "badLoaderTopic"

  # config for NSQ

  # Channel name for NSQ source
  # If more than one application reading from the same NSQ topic at the same time,
  # all of them must have unique channel name for getting all the data from the same topic
  # channelName = "{{nsqSourceChannelName}}"

  # Host name for NSQ tools
  # host = "{{nsqHost}}"

  # HTTP port for nsqd
  # port = {{nsqdPort}}

  # HTTP port for nsqlookupd
  # lookupPort = {{nsqlookupdPort}}

  # "LATEST": most recent data.
  # "TRIM_HORIZON": oldest available data.
  # "AT_TIMESTAMP": Start from the record at or after the specified timestamp
  # Note: This only affects the first run of this application on a stream.
  initialPosition = "{{initialPosition}}"
  initialPosition = ${?QUEUE_INITIAL_POSITION}

  # Need to be specified when initial-position is "AT_TIMESTAMP".
  # Timestamp format need to be in "yyyy-MM-ddTHH:mm:ssZ".
  # Ex: "2017-05-17T10:00:00Z"
  # Note: Time need to specified in UTC.
  initialTimestamp = "{{initialTimestamp}}"
  initialTimestamp = ${?QUEUE_INITIAL_TIMESTAMP}

  # Maximum number of records to get from Kinesis per call to GetRecords
  maxRecords = "{{kinesisMaxRecords}}"
  maxRecords = ${?QUEUE_MAX_RECORDS}

  # Region where the Kinesis stream is located
  region = "{{kinesisRegion}}"
  region = ${?AWS_REGION}

  ## Optional endpoint url configuration to override aws kinesis endpoints,
  ## this can be used to specify local endpoints when using localstack
  # customEndpoint = {{kinesisEndpoint}}

  ## Optional endpoint url configuration to override aws dyanomdb endpoints,
  ## this can be used to specify local endpoints when using localstack
  # dynamodbCustomEndpoint = "http://localhost:4569"

  # "appName" is used for a DynamoDB table to maintain stream state.
  # You can set it automatically using: "SnowplowElasticsearchSink-${sink.kinesis.in.stream-name}"
  appName = "loader"
  appName = ${?LOADER_APP_NAME}

  # Initial RCU/WCU are optional. These are the settings for Dynamodb lease table throughput initalization
  initialRCU = "{{leastTableInitialRCU}}"
  initialRCU = ${?LEASE_TABLE_INITIAL_RCU}
  initialWCU = "{{leastTableInitialWCU}}"
  initialWCU = ${?LEASE_TABLE_INITIAL_WCU}
}

# Common configuration section for all stream sources
streams {
  inStreamName = "outEnriched"
  inStreamName = ${?INPUT_QUEUE}

  # Stream for enriched events which are rejected by Elasticsearch
  outStreamName = ""
  outStreamName = ${?OUTPUT_QUEUE}

  # Events are accumulated in a buffer before being sent to Elasticsearch.
  # The buffer is emptied whenever:
  # - the combined size of the stored records exceeds byteLimit or
  # - the number of stored records exceeds recordLimit or
  # - the time in milliseconds since it was last emptied exceeds timeLimit
  buffer {
    byteLimit = 10000 # Not supported by NSQ, will be ignored
    byteLimit = ${?LOADER_BUFFER_BYTE_THRESHOLD}
    recordLimit = 2
    recordLimit = ${?LOADER_BUFFER_RECORD_THRESHOLD}
    timeLimit = 1000
    timeLimit = ${?LOADER_BUFFER_TIME_THRESHOLD} # Not supported by NSQ, will be ignored
  }
}

# Sinks

postgres {
  server = ""
  server = ${?POSTGRES_SERVER}
  port = "5432"
  port = ${?POSTGRES_PORT}
  databaseName = "postgres"
  databaseName = ${?POSTGRES_DATABASE_NAME}
  username = "postgres"
  username = ${?POSTGRES_USERNAME}
  password = "postgres"
  password = ${?POSTGRES_PASSWORD}
  schemas = "/home/knoldus/Desktop/schemas/"
  schemas = ${?POSTGRES_SCHEMAS}
  table = "atomic.events"
  table = ${?POSTGRES_TABLE}
  shardTableDateField = "derived_tstamp"
  shardTableDateField = ${?SHARD_DATE_FIELD}
  shardTableDateFormat = "yyyy_MM_dd"
  shardTableDateFormat = ${?SHARD_DATE_FORMAT}
  filterAppId = ""
  filterAppId = ${?KINESIS_SINK_APP_ID_FILTER}
}

elasticsearch {

  # Events are indexed using an Elasticsearch Client
  # - endpoint: the cluster endpoint
  # - port: the port the cluster can be accessed on
  #   - for http this is usually 9200
  #   - for transport this is usually 9300
  # - username (optional, remove if not active): http basic auth username
  # - password (optional, remove if not active): http basic auth password
  # - max-timeout: the maximum attempt time before a client restart
  # - ssl: if using the http client, whether to use ssl or not
  client {
    endpoint = "elasticsearch"
    endpoint = ${?ELASTICSEARCH_ENDPOINT}
    port = "9200"
    port = ${?ELASTICSEARCH_PORT}
    username = "elasticsearch"
    username = ${?ELASTICSEARCH_PASSWORD}
    password = "changeme"
    password = ${?ELASTICSEARCH_ENDPOINT}
    maxTimeout = "6000"
    maxTimeout = ${?ELASTICSEARCH_MAX_TIMEOUT}
    maxRetries = "10"
    maxRetries = ${?ELASTICSEARCH_MAX_RETRIES}
    ssl = "false"
    ssl = ${?ELASTICSEARCH_SSL}
  }

  # When using the AWS ES service
  # - signing: if using the http client and the AWS ES service you can sign your requests
  #    http://docs.aws.amazon.com/general/latest/gr/signing_aws_api_requests.html
  # - region where the AWS ES service is located
  aws {
    signing = "false"
    signing = ${?ELASTICSEARCH_SIGNING}
    region = "us-east-1"
    region = ${?AWS_REGION}
  }

  # index: the Elasticsearch index name
  # type: the Elasticsearch index type
  cluster {
    name = "cluster"
    name = ${?ELASTICSEARCH_CLUSTER_NAME}
    index = "events"
    index = ${?ELASTICSEARCH_INDEX}
    documentType = "enriched"
    documentType = ${?ELASTICSEARCH_DOCUMENT_TYPE}
    indexDateField = "{{shardDateField}}"
    indexDateField = ${?AWS_REGION}
    indexDateFormat = "{{shardDateFormat}}"
    indexDateFormat = ${?AWS_REGION}
  }
}

# In case you are loading data to S3.
s3 {
  bucket = "tgam-sophi-data"
  bucket = ${?BUCKET}
  s3AccessKey = ""
  s3AccessKey = ${?S3_ACCESS_KEY}
  s3SecretKey = ""
  s3SecretKey = ${?S3_SECRET_KEY}
  shardDateField = "derived_tstamp"
  shardDateField = ${?SHARD_DATE_FIELD}
  shardDateFormat = "yyyy_MM_dd"
  shardDateFormat = ${?SHARD_DATE_FORMAT}
}

# If sink is set to Kinesis
#kinesis {
# The following are used to authenticate for the Amazon Kinesis sink.
# If both are set to "default", the default provider chain is used
# (see http://docs.aws.amazon.com/AWSJavaSDK/latest/javadoc/com/amazonaws/auth/DefaultAWSCredentialsProviderChain.html)
# If both are set to "iam", use AWS IAM Roles to provision credentials.
# If both are set to "env", use environment variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY
#
# Optional:
#  kinesisArnRole = "arn:aws:iam::{accountId}:role/{role}"
#  kinesisStsRegion = us-east-1
#
#  arnRole can be added to enable assume role, make sure to include stsRegion in this case
#kinesisAccessKey = "{{KinesisAccessKey}}"
#kinesisSecretKey = "{{KinesisSecretKey}}"
#This filter is inclusive i.e. values mentioned here only will be allowed within the stream
#filterAppId = "{{FilterAppId}}"
# kinesisArnRole = "{{KinesisArnRole}}"
# kinesisStsRegion = "{{KinesisStsRegion}}"
#}

# Optional section for tracking endpoints
monitoring {
  snowplow {
    collectorUri = "0.0.0.0:7070"
    collectorUri = ${?COLLECTOR_URI}

    collectorPort = "7070"
    collectorPort = ${?COLLECTOR_PORT}

    appId = "loader"
    appId = ${?LOADER_APP_NAME}

    method = "/GET"
    method = ${?method}
  }
}

# Table mapping settings for snowplow loader
mappingTable {
  "contexts_com_snowplowanalytics_snowplow_web_page_1" = "contexts_com_snowplow_web_page_1",
  "contexts_com_snowplowanalytics_snowplow_ua_parser_context_1" = "contexts_com_snowplow_ua_parser_1",
  "unstruct_event_com_snowplowanalytics_snowplow_uri_redirect_1" = "unstruct_com_snowplow_uri_redirect_1",
  "unstruct_event_com_snowplowanalytics_snowplow_link_click_1" = "unstruct_com_snowplow_link_click_1",
  "unstruct_event_com_snowplowanalytics_snowplow_ad_click_1" = "unstruct_com_snowplow_ad_click_1",
  "unstruct_event_com_snowplowanalytics_snowplow_ad_impression_1" = "unstruct_com_snowplow_ad_impression_1",
  "unstruct_event_com_snowplowanalytics_snowplow_ad_conversion_1" = "unstruct_com_snowplow_ad_conversion_1",
  "unstruct_com_snowplowanalytics_snowplow_social_interaction_1" = "unstruct_com_snowplow_social_interaction_1",
}

deduplicationCache {
  deduplicationEnabled = "false"
  deduplicationEnabled = ${?DEDUPLICATION_ENABLED}
  deduplicationField = "event_fingerprint"
  deduplicationField = ${?DEDUPLICATION_FIELD}
  sizeLimit = "1000"
  sizeLimit = ${?DEDUPLICATION_SIZE_LIMIT}
  timeLimit = "3600"
  timeLimit = ${?DEDUPLICATION_TIME_LIMIT}

}